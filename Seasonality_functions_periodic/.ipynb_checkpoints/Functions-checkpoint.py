import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd
import torch
import arviz as az
from pyro import clear_param_store
import pyro.contrib.gp as gp
from pyro.nn import PyroSample
import pyro.distributions as dist
from pyro.infer import MCMC, NUTS, Predictive,HMC
from bokeh.models import Band, ColumnDataSource,Slope, Div,Label
from sklearn.metrics import r2_score,root_mean_squared_error
from bokeh.plotting import figure, show,output_file, save
from bokeh.transform import factor_cmap, factor_mark
from bokeh.palettes import Spectral
from bokeh.io import curdoc,output_notebook,export_png
from bokeh.layouts import column,gridplot
from scipy.stats import norm
import torch.multiprocessing as mp
from loguru import logger
import sys
from torchquad import Simpson,set_up_backend
from scipy.stats import spearmanr,normaltest
# Define the seed value
seed = 42
# Set seed for PyTorch
torch.manual_seed(seed)
# Set seed for CUDA (if using GPUs)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)  # For multi-GPU setups
# Set seed for NumPy
np.random.seed(seed)
# Ensure deterministic behavior for PyTorch operations
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
def make_Xy(data,X,y,device):
    
   # Remove rows from the dataframe where the target variable y has missing values
    data=data.dropna(subset=y)
    
    # Extract selected features, drop missing values, convert to float64, and move to specified device
    X_tensor = torch.tensor(data.loc[:,X].dropna().values.astype("float64")).to(device)
    
     # Extract the target variable, convert to double precision tensor, and move to specified device
    y_tensor=torch.tensor(data.loc[:,y].values).double().to(device)
    
    # Return both the feature tensor and the target tensor
    return X_tensor,y_tensor
def model(X,y,device,sigma_prior=None):
    
    # Clear the global Pyro parameter store to avoid interference from previous runs
    clear_param_store()
    
    # Check if a sigma prior is provided; if not, use the standard deviation of the target variable
    if sigma_prior ==None:
        # Clone and detach the standard deviation of y to use as a default prior
        sigma_prior=y.std().detach().clone()
    else:
        # Use the user-provided sigma prior value
        sigma_prior=sigma_prior
    # Initialize a Periodic kernel for the Gaussian Process with 1 input dimension
    
    rbf=gp.kernels.Periodic(input_dim=1)
    # Set the period of the kernel to the maximum value found in the features X
    
    rbf.period=torch.tensor(X.max())
    # Define a HalfNormal prior for the kernel variance based on the mean of y
    
    rbf.variance = PyroSample(dist.HalfNormal(y.mean()))
    # Define a HalfNormal prior for the lengthscale with a scale parameter of 2.0
    
    rbf.lengthscale = PyroSample(dist.HalfNormal(torch.tensor(2.)))
    # Initialize the Gaussian Process Regression model and move it to the specified device (CPU/GPU)
    
    gpr = gp.models.GPRegression(X,y, rbf).to(device)
    # Define a HalfNormal prior for the observation noise using the sigma_prior
    
    gpr.noise = PyroSample(dist.HalfNormal(sigma_prior))
    # Return both the NUTS sampler kernel and the GPR model object
    nuts_kernel = NUTS(gpr.model)
    
    # Return both the NUTS sampler kernel and the GPR model object
    return nuts_kernel,gpr
def train_model(X,nuts_kernel,gpr,device,steps_num=2000):
    # Set the multiprocessing start method to 'spawn' to ensure compatibility with PyTorch and CUDA
    mp.set_start_method('spawn', force=True)

    # Initialize the MCMC sampler with the NUTS kernel, setting warmup, samples, and 4 parallel chains
    mcmc = MCMC(nuts_kernel,warmup_steps=steps_num, num_samples=steps_num,num_chains=4)

    # Execute the MCMC sampling process to train the model
    mcmc.run()
    
    # Extract 500 samples from the posterior distribution generated by the MCMC run
    posterior_samples = mcmc.get_samples(500)

   # Generate posterior predictive distributions for the features X using the sampled parameters
    posterior_predictive= Predictive(gpr, posterior_samples)(X)

    # Generate 500 samples from the prior predictive distribution before incorporating data
    prior = Predictive(gpr, num_samples=500)(X)
    
    # Convert MCMC results, prior, and posterior predictive samples into an ArviZ InferenceData object
    pyro_data = az.from_pyro(mcmc,prior=prior,posterior_predictive=posterior_predictive)
    
    # Return the ArviZ data object and the fitted Gaussian Process Regression model
    return pyro_data,gpr

def gaussian_plot(x,model,device):
    
    # Convert dataframe values to a float64 torch tensor and move them to the specified device
    linmod=torch.from_numpy(data.values.astype("float64")).to(device)

    # Perform a forward pass on the GP model to get the mean and the full covariance matrix
    model_y,model_y_std=model(linmod,full_cov=True)
    
    # Move the mean and the standard deviation (sqrt of diagonal) to CPU and convert to numpy
    model_np=model_y.cpu().detach().numpy().copy()
    model_std_np=model_y_std.diag().sqrt().cpu().detach().numpy().copy()

    # Calculate the lower bound of the predictive interval (mean minus one standard deviation)
    lower = model_np - model_std_np

    # Calculate the upper bound of the predictive interval (mean plus one standard deviation)
    upper = model_np + model_std_np

    # Create a pandas DataFrame containing the predictions and bounds, then set the index to x
    data_plot=pd.DataFrame([model_np,lower,upper],index=["y","lower","upper"]).T.set_index(x)

    # Return a Bokeh ColumnDataSource for plotting and the raw pandas DataFrame
    return ColumnDataSource(data_plot.reset_index()),data_plot

def roll_week(data,resolution,week,particle):
    
    # Initialize an empty list to store the rolling mean values
    mean=[]
    
    # Initialize an empty list to store the rolling standard deviation values
    std=[]

    # Create an array of evenly spaced points from 0 to 52.5 based on the specified resolution
    x=np.linspace(0,52.5,resolution,endpoint=True)

    # Sort the input dataframe by the 'corrected_week' column to ensure temporal order
    sort=data.sort_values("corrected_week")

    # Iterate through each point in the generated range x
    for i in x:
        # Calculate the mean of the particle variable within a window of +/- 'week' around point i
        mean.append(sort.loc[(i-week<sort.corrected_week)&(i+week>sort.corrected_week),particle].mean())

        # Calculate the standard deviation of the particle variable within the same window
        std.append(sort.loc[(i-week<sort.corrected_week)&(i+week>sort.corrected_week),particle].std())
        
    # Convert the mean and standard deviation lists into pandas DataFrames indexed by x
    mean_1,std_1=pd.DataFrame(mean,index=x),pd.DataFrame(std,index=x)

    # Calculate the lower bound of the standard deviation interval
    lower_std = mean_1 - std_1

    # Calculate the upper bound of the standard deviation interval
    upper_std = mean_1 + std_1

    # Concatenate the mean, lower, and upper bounds into a single DataFrame along columns
    mean=pd.concat([mean_1,lower_std,upper_std],axis=1)
    
    # Rename the columns to indicate the mean value and the confidence bounds
    mean.columns=["y","lower","upper"]
    
    # Explicitly set the index of the final DataFrame to the points in x
    mean=mean.set_index(x)
    
    # Convert the resulting DataFrame into a Bokeh ColumnDataSource for visualization
    mean1 = ColumnDataSource(mean)
    
    # Return both the Bokeh data source and the pandas DataFrame
    return mean1,mean

def integrand(x):
    
    # Squeeze the input dimension to match model expectations and move it to the correct device
    x_input = x.squeeze(-1).to(device)
    
    # Disable gradient calculations to save memory and compute time during integration
    with torch.no_grad():
        # Perform a forward pass on the model without full covariance to get the mean
        output = model(x_input, full_cov=False)

        # Unpack the output: if it's a tuple, take the first element (mean)
        mean,_ = output if isinstance(output, torch.Tensor) else output
    #return the mean
    return mean

def integrate_gp_mean(model, domain,i, points=500):
    
    # Remove the default loguru logger and add a new one that only displays WARNING level or higher
    logger.remove()
    logger.add(sys.stderr, level="WARNING") 
    
    # Initialize the Simpson's rule integrator
    integrator = Simpson()
    
    # Identify the device (CPU or GPU) by checking where the model parameters are located
    device = next(model.parameters()).device
    
    def integrand(x):
        # Squeeze the input dimension to match model expectations and move it to the correct device

        x_input = x.squeeze(-1).to(device)
        with torch.no_grad():
            # Vi tager kun middelvÃ¦rdien [0]
            output = model(x_input, full_cov=False)
            mean,_ = output if isinstance(output, torch.Tensor) else output
            return mean

    # Execute the numerical integration using Simpson's rule over the specified domain and N points
    result = integrator.integrate(
        integrand, 
        dim=1, 
        N=points, 
        integration_domain=domain, 
        backend="torch"
    )
    #return the scaling value 
    return (result.item()/(abs(domain[0][0]-domain[0][1])*model(torch.tensor(i).double().reshape(1).to(device))[0])).cpu().detach().numpy()[0]

    
def scale_season(data,model,period,name="scale"):

    # Store scaling values 
    X=[]
    # Fix domine dependent on forward or backwards scaling of the year
    for i in data.corrected_week: 
        # Negative period 
        if np.sign(period)==-1:
            # Return netative domain 
            domain=[[i+period,i]]
            
        # Postive period
        else: 
           # Return positive domain  
            domain=[[i,i+period]]
            
         # append the stored scaling values    
        X.append(integrate_gp_mean(model,domain,i))
        
    # Store the scaling values in the original dataframe     
    data[name]=X
    
    # Return the Dataframe
    return data
def gaussian_plot(data,model,resolution,input_x="corrected_week"):
    
    # Contruct a linspace from start to end of year
    linmod=torch.linspace(0,52.3,resolution).to(device)

    # Return  mean and stantard deviation from the model over the year.
    model_y,model_y_std=model(linmod,full_cov=True)
    
    # Convert mean, std tensors to numpy
    model_np=model_y.cpu().detach().numpy().copy()
    model_std_np=model_y_std.diag().sqrt().cpu().detach().numpy().copy()

    # Calculate lower error of the model over the year.
    lower = model_np - model_std_np

    # Calculate upper error of the model over the year.
    upper = model_np + model_std_np

    # Construct dataframe with all the values
    data_plot=pd.DataFrame([model_np,lower,upper],index=["y","lower","upper"]).T.set_index(linmod.cpu().detach().numpy())
    # Concert to Bohek Datasource and return it togetehr with dataframe 
    return ColumnDataSource(data_plot.reset_index()),data_plot

def boundary_roll(data,step=2):
    # Extend steps to two std 
    step=step*2
    
    # Put week of the year as index
    data=data.set_index("corrected_week")
    
    #Store end weeks
    data_end=data.loc[data.index>data.index.max()-step]
    
    # Put the weeks infront of the year 
    data_end.index=data_end.index-data_end.index.max()

    # Store start weeks of the year
    data_start=data.loc[data.index<data.index.min()+step]
    
    # Put the weeks in the end of the year 
    data_start.index=data.index.max()+data_start.index

    # return new dataframe with periodic condition 2 steps in end and start
    return pd.concat([data,data_end,data_start]).reset_index()
    
def roll_week(data,particle,resolution,input_x="corrected_week",step=2):
    
    # Make list to store mean
    mean=[]
    
    # Make list to store std
    std=[]
    
    # Making a linspace over the year
    x=np.linspace(0,52.3,resolution,endpoint=True)

    # Making the datset periodic within 2 weeks times two in both ends  
    bound_data=boundary_roll(data,step=step)

    # Iterate over all points
    for i in x:
        
        # Calulate the weight by the smaples distance to the samples with a std of step
        weight=norm.pdf(bound_data.corrected_week-i,step)

        # Calculate the mean staled with the distance to points 
        mean_var=(bound_data[particle].values*weight).sum()/weight.sum()

        # store the mean 
        mean.append(mean_var)

        # store the std of the mean 
        std.append(np.sqrt(1/(weight.sum()-1)*(weight*(bound_data[particle].values-mean_var)**2).sum()))

    # Save the mean and staandard  deviation in a dataFrame
    mean_1,std_1=pd.DataFrame(mean,index=x),pd.DataFrame(std,index=x)

    # Calculate lower bound 
    lower_std = mean_1 - std_1

    # Calculate upper bound
    upper_std = mean_1 + std_1

    # store mean, upper and lower bound
    mean=pd.concat([mean_1,lower_std,upper_std],axis=1)

    # Change name of columns
    mean.columns=["y","lower","upper"]

    # index is the week
    mean=mean.set_index(x)

    # Construct bokeh datasource
    mean1 = ColumnDataSource(mean)

    # Return Bokeh  Datasource and pandas Datafarme
    return mean1,mean
    
def plot(particle,title,train,test,model,x_label=r"$$Week \ of \ the \ year$$",y_label=r"$$\frac{\mu g}{m^3} $$",diffrence=10,step=2,text=13,resolution=500,w=1000,h=500):
    
    # Which tools should be in the bokeh plot 
    TOOLS="hover,crosshair,wheel_zoom,zoom_in,zoom_out,undo,redo,reset,tap,save,box_select,examine,help"


    # Remove nan in train 
    train=train.dropna(subset=particle)

    # Remove nan in test 
    test=test.dropna(subset=particle)

    # Contrust periodic means for train 
    mean_train,roll_train=roll_week(train,particle,resolution,step=step)

    # Contrust periodic means for test 
    mean_test,roll_test=roll_week(test,particle,resolution,step=step)

    #Make Dataset to plot the means  
    model_plot,model_dataframe=gaussian_plot(train,model,resolution)

    # What is the range of the plot x axis 
    x_range_start=(model_dataframe.index.min(), model_dataframe.index.max())

    # What is the range of the plot y axis 
    y_range_start=(roll_train.y.min()-diffrence,roll_train.y.max()+diffrence)

# Calculate the R^2 for train, test ,train vs test between the model and the periodic means
    r2_train=r2_score(roll_train.y,model_dataframe.y)

    r2_test=r2_score(roll_test.y,model_dataframe.y)

    r2_train_vs_test=r2_score(roll_test.y,roll_train.y)

# Calculate the RMSE for train, test ,train vs test between the model and the periodic means
    rmse_train=root_mean_squared_error(roll_train.y,model_dataframe.y)

    rmse_test=root_mean_squared_error(roll_test.y,model_dataframe.y)

    rmse_train_vs_test=root_mean_squared_error(roll_test.y,roll_train.y)
     # Calcualte the R^2 between the test samples and the model.
    r2=r2_score(test[particle],model(torch.from_numpy(test.corrected_week.values.astype("float64")).to(device))[0].cpu().detach().numpy())

    # Construct figure
    p = figure(tools=TOOLS,x_range=x_range_start,y_range=y_range_start,width=w, height=h);

    # choose text size 
    p.title.text_font_size = '15pt'

    # Set title
    p.title.text =title;  

    # choose tranparentcy 
    p.ygrid.grid_line_alpha=0.5;

    # Plot the weighted periodic mean for train
    p.line(roll_train.index, roll_train.y, line_width=3,color="green");

    # Plot the weighted periodic mean for test
    p.line(roll_test.index, roll_test.y, line_width=3,color="orange");
   
    # Plot the weighted periodic mean for the model
    p.line(model_dataframe.index, model_dataframe.y, line_width=3,color="red");
    
    #plot train datapoints 
    p.scatter(train.corrected_week, y=train[particle], color="blue", marker="dot", size=20, alpha=0.4);

    # plot the std of the weighted periodic mean
    band_data= Band(base="index", lower="lower", upper="upper",source=mean_train, fill_color="red", line_color="black",fill_alpha=0.2);
    p.add_layout(band_data);

    # Plot the std of the model  
    band_model = Band(base="index", lower="lower", upper="upper",source=model_plot,fill_alpha=0.5, fill_color="blue", line_color="black");
    p.add_layout(band_model);

    # Put in train R^2 vs the model
    train_label=Label(x=model_dataframe.index.max()-model_dataframe.index.max()/2,y=y_range_start[1]-diffrence/4,text=r"$$Train \ R^2$$= "+str(round(r2_train,2)),text_font_size=str(text)+"pt",text_color="green")
    p.add_layout(train_label)

    # Put in test R^2 vs the model 
    test_label=Label(x=model_dataframe.index.max()-model_dataframe.index.max()/2,y=y_range_start[1]-diffrence*2/4,text=r"$$Test \ R^2$$= "+str(round(r2_test,2)),text_font_size=str(text)+"pt",text_color="orange")
    p.add_layout(test_label)

    # Put in train vs test mean R^2 
    test_train=Label(x=model_dataframe.index.max()-model_dataframe.index.max()/2,y=y_range_start[1]-diffrence*3/4,text=r"$$Test vs train \ R^2$$= "+str(round(r2_train_vs_test,2)),text_font_size=str(text)+"pt",text_color="blue")
    p.add_layout(test_train)

    # Put in train RMSE vs the model 
    train_label_rmse=Label(x=model_dataframe.index.max()-model_dataframe.index.max()*3/4,y=y_range_start[1]-diffrence/4,text=r"$$Train \ RMSE$$= "+str(round(rmse_train,2)),text_font_size=str(text)+"pt",text_color="green")
    p.add_layout(train_label_rmse)
    
     # Put in test RMSE vs the model 
    test_label_rmse=Label(x=model_dataframe.index.max()-model_dataframe.index.max()*3/4,y=y_range_start[1]-diffrence*2/4,text=r"$$Test \ RMSE$$= "+str(round(rmse_test,2)),text_font_size=str(text)+"pt",text_color="orange")
    p.add_layout(test_label_rmse)

     # Put in test vs train RMSE vs the model 
    test_train_rmse=Label(x=model_dataframe.index.max()-model_dataframe.index.max()*3/4,y=y_range_start[1]-diffrence*3/4,text=r"$$Test vs train \ RMSE$$= "+str(round(rmse_train_vs_test,2)),text_font_size=str(text)+"pt",text_color="blue")
    p.add_layout(test_train_rmse)

    # Put in R^2 for the model versus sample 
    household=Label(x=model_dataframe.index.max()-model_dataframe.index.max()*1/3,y=y_range_start[1]-diffrence/4,text=r"$$Household \ R^2$$= "+str(round(r2,2)),text_font_size=str(text)+"pt",text_color="red")
    p.add_layout(household)

    # Change direction of labels
    p.yaxis.axis_label_orientation  = 0

    # Change xlabel name
    p.xaxis.axis_label = x_label

    # Change ylabel name
    p.yaxis.axis_label = y_label
    
    # Change size of x label
    p.xaxis.axis_label_text_font_size = "30px";
    # Change size of y  label
    p.yaxis.axis_label_text_font_size = "30px";
    return p